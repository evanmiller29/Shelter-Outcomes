{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "train['type'] = 'train'\n",
    "\n",
    "test = pd.read_csv('test.csv')\n",
    "test['type'] = 'test'\n",
    "test['OutcomeSubtype'] = ''\n",
    "test['OutcomeType'] = ''\n",
    "\n",
    "def data_import(df1, df2):\n",
    "\n",
    "    df = (df1.append(df2)\n",
    "         .rename(columns=str.lower))\n",
    "    \n",
    "    # functions to get new parameters from the column\n",
    "    def get_sex(x):\n",
    "        x = str(x)\n",
    "        if x.find('Male') >= 0: return 'male'\n",
    "        if x.find('Female') >= 0: return 'female'\n",
    "        return 'unknown'\n",
    "    \n",
    "    def get_neutered(x):\n",
    "        x = str(x)\n",
    "        if x.find('Spayed') >= 0: return 'neutered'\n",
    "        if x.find('Neutered') >= 0: return 'neutered'\n",
    "        if x.find('Intact') >= 0: return 'intact'\n",
    "        return 'unknown'\n",
    "\n",
    "    df['sex'] = df.sexuponoutcome.apply(get_sex)\n",
    "    df['neutered'] = df.sexuponoutcome.apply(get_neutered)\n",
    "    \n",
    "    def get_mix(x):\n",
    "        x = str(x)\n",
    "        if x.find('Mix') >= 0: return 'mix'\n",
    "        return 'not'\n",
    "\n",
    "    df['mix'] = df.breed.apply(get_mix)\n",
    "    \n",
    "    \n",
    "    def calc_age_in_years(x):\n",
    "        x = str(x)\n",
    "        if x == 'nan': return np.nan\n",
    "        age = int(x.split()[0])\n",
    "        if x.find('year') > -1: return age \n",
    "        if x.find('month')> -1: return age / 12.\n",
    "        if x.find('week')> -1: return age / 52.\n",
    "        if x.find('day')> -1: return age / 365.\n",
    "        else: return np.nan\n",
    "    \n",
    "    df['ageinyears'] = df.ageuponoutcome.apply(calc_age_in_years)\n",
    "    \n",
    "    # Creating some more date variables\n",
    "\n",
    "    from datetime import datetime\n",
    "\n",
    "    df['datetime'] = pd.to_datetime(df.datetime)\n",
    "    df['year'] = df['datetime'].map(lambda x: x.year).astype(str)\n",
    "    df['year'] = df['datetime'].map(lambda x: x.month).astype(str)\n",
    "    df['wday'] = df['datetime'].map(lambda x: x.dayofweek).astype(str)\n",
    "    \n",
    "    def has_name(x):\n",
    "        if x == 'Nameless': return 0\n",
    "        else: return 1\n",
    "    \n",
    "    df['hasname'] = df['name'].map(has_name)\n",
    "    \n",
    "    drop_cols = ['animalid', 'datetime', 'name', 'ageuponoutcome', 'sexuponoutcome', 'outcomesubtype']\n",
    "\n",
    "    df.drop(drop_cols, axis=1, inplace=True)\n",
    "\n",
    "    df['mix'] = df['breed'].str.contains('Mix').astype(int)\n",
    "\n",
    "    df['color_simple'] = df.color.str.split('/| ').str.get(0)\n",
    "    df.drop(['breed', 'color'], axis = 1 , inplace = True)\n",
    "    \n",
    "    # Using mean imputation of missing values. Can build on if necessary\n",
    "    \n",
    "    df['ageinyears'] = df.ageinyears.fillna(df.ageinyears.mean())\n",
    "    \n",
    "    # Just using training data for model building\n",
    "    \n",
    "    return(df)\n",
    "\n",
    "df = data_import(train, test)\n",
    "\n",
    "def prep_data(dataframe, type):\n",
    "    \n",
    "    df = dataframe.copy()\n",
    "    df.drop('id', axis = 1, inplace = True)\n",
    "    df = df.loc[df.type == type,:]\n",
    "    df.drop('type', axis = 1, inplace = True)\n",
    "    \n",
    "    # Encoding labels\n",
    "    \n",
    "    y = df['outcometype'].values\n",
    "    \n",
    "    from sklearn import preprocessing\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    \n",
    "    le.fit(y)\n",
    "    \n",
    "    y = le.transform(y)\n",
    "    X = df\n",
    "    X.drop(['outcometype'], axis=1, inplace=True)\n",
    "    \n",
    "    X = pd.get_dummies(X)\n",
    "    \n",
    "    from sklearn.preprocessing import Imputer\n",
    "\n",
    "    # Imputing missing values\n",
    "    imp = Imputer(missing_values=np.nan, strategy='mean', axis=0)\n",
    "    imp.fit(X)\n",
    "\n",
    "    X = imp.transform(X)\n",
    "    \n",
    "    return(X, y, le)\n",
    "\n",
    "X, y, le = prep_data(df, 'train')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "best n_components by PCA CV = 48\n",
      "best n_components by FactorAnalysis CV = 11\n",
      "best n_components by PCA MLE = 58\n"
     ]
    }
   ],
   "source": [
    "from scipy import linalg\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "n_components = np.arange(0, n_features, 1)  # options for n_components\n",
    "\n",
    "def compute_scores(X):\n",
    "    \n",
    "    # Fit the models\n",
    "    pca = PCA()\n",
    "    fa = FactorAnalysis()\n",
    "\n",
    "    pca_scores, fa_scores = [], []\n",
    "    for n in n_components:\n",
    "        pca.n_components = n\n",
    "        fa.n_components = n\n",
    "        pca_scores.append(np.mean(cross_val_score(pca, X)))\n",
    "        fa_scores.append(np.mean(cross_val_score(fa, X)))\n",
    "\n",
    "    return pca_scores, fa_scores\n",
    "\n",
    "for X in [(X_train)]:\n",
    "    pca_scores, fa_scores = compute_scores(X)\n",
    "    n_components_pca = n_components[np.argmax(pca_scores)]\n",
    "    n_components_fa = n_components[np.argmax(fa_scores)]\n",
    "\n",
    "    pca = PCA(n_components='mle')\n",
    "    pca.fit(X)\n",
    "    n_components_pca_mle = pca.n_components_\n",
    "\n",
    "    print(\"best n_components by PCA CV = %d\" % n_components_pca)\n",
    "    print(\"best n_components by FactorAnalysis CV = %d\" % n_components_fa)\n",
    "    print(\"best n_components by PCA MLE = %d\" % n_components_pca_mle)\n",
    "    \n",
    "pca = PCA(n_components= n_components_pca)\n",
    "X_PC_48 = pca.fit(X_train).transform(X_train)\n",
    "\n",
    "pca = PCA(n_components= n_components_pca_mle)\n",
    "X_PC_mle = pca.fit(X_train).transform(X_train)\n",
    "\n",
    "fa = FactorAnalysis(n_components= n_components_fa)\n",
    "X_FA_11 = fa.fit(X_train).transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Decision Tree\",\n",
    "         \"Random Forest\", \"AdaBoost\", \"Naive Bayes\", \"Linear Discriminant Analysis\",\n",
    "         \"Quadratic Discriminant Analysis\", \"AdaBoosted decision trees\"]\n",
    "\n",
    "knn = KNeighborsClassifier(5)\n",
    "linear_svm = SVC(kernel=\"linear\", C=0.025)\n",
    "rbf_svm = SVC(gamma=2, C=1)\n",
    "d_tree = DecisionTreeClassifier(max_depth=5)\n",
    "rf = RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)\n",
    "adaboost = AdaBoostClassifier()\n",
    "naive_bayes = GaussianNB()\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "adaboost_rf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=5),\n",
    "                         algorithm=\"SAMME\",\n",
    "                         n_estimators=200)\n",
    "\n",
    "classifiers = [\n",
    "    knn,\n",
    "    linear_svm,\n",
    "    rbf_svm,\n",
    "    d_tree,\n",
    "    rf,\n",
    "    adaboost,\n",
    "    naive_bayes,\n",
    "    lda,\n",
    "    qda,\n",
    "    adaboost_rf\n",
    "]\n",
    "\n",
    "def est_score(X, y):\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        clf.fit(X, y)\n",
    "        score = clf.score(X, y)\n",
    "        print(name, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbors 0.702308354222\n",
      "Linear SVM 0.62654794418\n",
      "RBF SVM 0.877735792585\n",
      "Decision Tree 0.63698604512\n",
      "Random Forest 0.607579782259\n",
      "AdaBoost 0.635639193385\n",
      "Naive Bayes 0.0423135919787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EvanMi\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:387: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\EvanMi\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:688: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Discriminant Analysis 0.628119271204\n",
      "Quadratic Discriminant Analysis 0.0178457854764\n",
      "AdaBoosted decision trees 0.620561936473\n"
     ]
    }
   ],
   "source": [
    "# Looking at the original transformation\n",
    "est_score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbors 0.702196116578\n",
      "Linear SVM 0.626697594373\n",
      "RBF SVM 0.877698380037\n",
      "Decision Tree 0.636200381608\n",
      "Random Forest 0.540087545363\n",
      "AdaBoost 0.611208799431\n",
      "Naive Bayes 0.174155411725\n",
      "Linear Discriminant Analysis 0.628119271204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EvanMi\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:688: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quadratic Discriminant Analysis 0.0182199109581\n",
      "AdaBoosted decision trees 0.604100415279\n"
     ]
    }
   ],
   "source": [
    "# Looking at the first 4 principal components\n",
    "est_score(X_PC_48, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbors 0.681132851959\n",
      "Linear SVM 0.626622769277\n",
      "RBF SVM 0.823337947548\n",
      "Decision Tree 0.622582214075\n",
      "Random Forest 0.580493097385\n",
      "AdaBoost 0.614052153092\n",
      "Naive Bayes 0.415017396835\n",
      "Linear Discriminant Analysis 0.627707733174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EvanMi\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:688: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quadratic Discriminant Analysis 0.0184817987953\n",
      "AdaBoosted decision trees 0.593587489244\n"
     ]
    }
   ],
   "source": [
    "# Looking at the MLE determined PC transformation\n",
    "est_score(X_PC_mle, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbors 0.680870964121\n",
      "Linear SVM 0.607841670096\n",
      "RBF SVM 0.607841670096\n",
      "Decision Tree 0.607841670096\n",
      "Random Forest 0.607841670096\n",
      "AdaBoost 0.607841670096\n",
      "Naive Bayes 0.52830259269\n",
      "Linear Discriminant Analysis 0.607841670096\n",
      "Quadratic Discriminant Analysis 0.402895731228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EvanMi\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:387: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\EvanMi\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:688: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoosted decision trees 0.607841670096\n"
     ]
    }
   ],
   "source": [
    "# Looking at the determined FA transformation\n",
    "# For some weird reason a large amount of these are the same.. FML\n",
    "est_score(X_FA_11, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "# From the above OOB investigation we definitely want ot look at RBF SVMs. This code is taken from:\n",
    "# http://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# Utility function to move the midpoint of a colormap to be around\n",
    "# the values of interest.\n",
    "\n",
    "class MidpointNormalize(Normalize):\n",
    "\n",
    "    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n",
    "        self.midpoint = midpoint\n",
    "        Normalize.__init__(self, vmin, vmax, clip)\n",
    "\n",
    "    def __call__(self, value, clip=None):\n",
    "        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n",
    "        return np.ma.masked_array(np.interp(value, x, y))\n",
    "\n",
    "def svm_rbf(X, y):\n",
    "\n",
    "    ##############################################################################\n",
    "    # Train classifiers\n",
    "    #\n",
    "    # For an initial search, a logarithmic grid with basis\n",
    "    # 10 is often helpful. Using a basis of 2, a finer\n",
    "    # tuning can be achieved but at a much higher cost.\n",
    "\n",
    "    C_range = np.logspace(-2, 10, 13)\n",
    "    gamma_range = np.logspace(-9, 3, 13)\n",
    "    param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "    cv = StratifiedShuffleSplit(y, n_iter=5, test_size=0.2, random_state=42)\n",
    "    grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)\n",
    "    grid.fit(X, y)\n",
    "\n",
    "    print(\"The best parameters are %s with a score of %0.2f\"\n",
    "          % (grid.best_params_, grid.best_score_))\n",
    "\n",
    "    C_2d_range = [1e-2, 1, 1e2]\n",
    "    gamma_2d_range = [1e-1, 1, 1e1]\n",
    "    classifiers = []\n",
    "    for C in C_2d_range:\n",
    "        for gamma in gamma_2d_range:\n",
    "            clf = SVC(C=C, gamma=gamma)\n",
    "            clf.fit(X, y)\n",
    "            classifiers.append((C, gamma, clf))\n",
    "\n",
    "    scores = [x[1] for x in grid.grid_scores_]\n",
    "    scores = np.array(scores).reshape(len(C_range), len(gamma_range))\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.subplots_adjust(left=.2, right=0.95, bottom=0.15, top=0.95)\n",
    "    plt.imshow(scores, interpolation='nearest', cmap=plt.cm.hot,\n",
    "               norm=MidpointNormalize(vmin=0.2, midpoint=0.92))\n",
    "    plt.xlabel('gamma')\n",
    "    plt.ylabel('C')\n",
    "    plt.colorbar()\n",
    "    plt.xticks(np.arange(len(gamma_range)), gamma_range, rotation=45)\n",
    "    plt.yticks(np.arange(len(C_range)), C_range)\n",
    "    plt.title('Validation accuracy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_rbf(X_PC_48, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
